---
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
source("include/globals.R")

# add TES in metafor
# add why multiple high p-values are surprising.
# https://daniellakens.blogspot.com/2015/05/after-how-many-p-values-between-0025.html

```


```{r}
knitr::opts_chunk$set(echo = TRUE)
library(bibtex)

```

# Bias detection {#bias}
# 偏倚检测 {#偏倚}

在整个研究过程中都可能会出现偏差。察觉并预防偏差的出现将会有很大的帮助。一些研究者建议对科学文献中的任何结论都持怀疑态度。例如，科学哲学家Deborah Mayo[-@mayo_statistical_2018]写道："面对最新的统计学新闻时，你的第一个问题应该是：这些结果是由于选择性报告、选择性挑选或者其他类似伎俩导致的吗？"。如果你在下一次参加的科学会议上问这个问题，可能会让自己不那么受欢迎，但忽视研究者或多或少有意地在他们的结论中引入偏差这一事实是非常幼稚的。

在引入偏差到科学研究的行为中,最极端的形式就是**研究不当行为**，包括伪造数据或结果、更改或剔除数据或结果，使得研究记录不能准确反映研究发现。例如，[AndrewWakefield](https://en.wikipedia.org/wiki/Andrew_Wakefield)在1998年撰写了一篇造假文章，声称麻疹、流行性腮腺炎和风疹(MMR)疫苗与自闭症有关。这篇文章在破坏了一些公众对疫苗的信任后，才终于在2010年被撤销。另一个例子来自心理学领域，[James Vicary](https://en.wikipedia.org/wiki/James_Vicary)进行的一项关于潜意识启示的研究。他声称，在电影院屏幕上闪现低于意识阈限的词语"吃爆米花"和"喝可乐"，爆米花和可乐的销售额分别增长了57.5%、18.1%. 然而，人们后来发现Vicary很可能进行了学术欺诈，因为没有任何证据表明确实进行过这样一项研究[@rogers_how_1992]。Retraction-Watch网站维护着一个[数据库](http://retractiondatabase.org)，跟踪记录了科学论文被撤销的原因，包括数据造假。虽然实际中数据造假的发生频率未知，但正如[研究诚信](#integrity)章节中所讨论的那样，我们应该可以预计至少有一小部分科学家对数据和结果进行过一次以上的捏造和篡改。

(ref:outlierlab)《The Dropout》中的一幕，讲述了Theranos公司虚假宣传拥有可以使用极少量血液进行血液检测的设备。在这一幕中，两名举报者在与他们的上司进行对峙，起因是他们被强迫删除不符合预期结果的数据。

```{r outliers, echo = FALSE, fig.cap="(ref:outlierlab)"}

if (knitr:::is_latex_output()) {
  knitr::include_graphics("images/dropout_outlier.png")
} else {
  knitr::include_graphics("images/dropout_outlier_small.gif")
}
```

另一类错误是统计报告错误，包括报告错误的自由度、将p=0.056报告为p\<0.05等[@nuijten_prevalence_2015]。虽然我们应该尽力避免错误，但每个人都会犯错，而随着数据和代码共享变得越来越普遍，能够更容易地检测出其他研究人员工作中的错误。正如Dorothy Bishop[-@bishop_fallibility_2018]所写的："随着开放科学越来越成为常态，我们将发现每个人会犯错误。科学家的声誉将不取决于他们的研究中是否存在缺陷，而是取决于在发现这些缺陷时他们如何回应。

[Statcheck](http://statcheck.io/)是一款自动从文章中提取统计数据并重新计算其p值的软件，统计数据需要按照美国心理学协会(APA)的指导方针报告。它会检查报告的统计数据内部是否一致：在给定的检验统计数据和自由度下，报告的p值是否准确？如果准确，那么犯错的可能性就较小(但这并不适用于逻辑错误!)，如果不准确，你应该检查统计检验中的所有信息是否准确。Statcheck不是完美的，它会导致第一类错误，即当实际上没有错误时，它将某些内容标记为错误，但它是一个易于使用的工具，可以在投稿之前用来检查文章。

有些数据的不一致性不太容易自动检测，但可以手动识别。例如，@brown_grim_2017表明，许多论文报告的平均值在样本量给定的情况下不可能出现（称为[*GRIM*](http://nickbrown.fr/GRIM)[ test](http://nickbrown.fr/GRIM)）。例如，Matti Heino在一篇博客文章[blog post](https://mattiheino.com/2016/11/13/legacy-of-psychology/)中注意到，Festinger和Carlsmith的经典研究表格中报告的三个平均值在数学上是不可能的。对于每个条件20个观测值和-5到5的标度，所有平均值应该以1/20的倍数或0.05结尾。以X.X8或X.X2结尾的三个平均值与报告的样本量和标度不一致。当然，这种不一致性可能是由于未报告某些问题的数据缺失引起的，但GRIM测试也已被用于揭示科学不端行为[scientific misconduct](https://en.wikipedia.org/wiki/GRIM_test)。

(ref:festingerlab) 1959年Festinger和Carlsmith所作报告中主要结果的表格截图。

```{r festinger, echo = FALSE, fig.cap="(ref:festingerlab)"}
knitr::include_graphics("images/festinger_carlsmith.png")
```

## Publication bias {#publicationbias}
## 出版偏见 {#publicationbias}

出版偏见是科学面临的最大挑战之一。**出版偏见**指有选择地提交和出版科学研究，通常基于结果是否有"统计显著"。科学文献被这些统计显著的结果所主导。同时，我们知道，研究人员进行的许多研究并没有产生显著的结果。当科学家们只能获得显著的结果，而不能获得所有的结果时，他们就缺乏对某一假设的证据的完整概述。在极端情况下，选择性报告会导致这样一种情况：在已发表的文献中，有数百个统计显著的结果，但因为有更多非显著的研究没有被分享，学术研究没有真正的效果。这就是所谓的**文件抽屉问题**，即非显著结果被藏在文件抽屉里（或者现在是电脑上的文件夹），不为科学界所了解。每个科学家都应该努力解决出版偏见，因为只要科学家不分享他们的所有结果，就很难了解什么是可能的事实，而且正如Greenwald[-@greenwald_consequences_1975]所指出的，这是一种违反道德的行为。

(ref:greenwaldlab) Greenwald,A.G.(1975)最后一句话的截图，这篇文章指出对无效假设偏见的后果。Psychological&Bulletin, 82(1), 1--20.

```{r greenwald, echo = FALSE, out.width  = '100%', fig.cap="(ref:greenwaldlab)"}
knitr::include_graphics("images/greenwald.png")
```

只有将你的所有研究成果提供给科学家同行，无论主要假设检验的*p*值是多少，才能解决出版偏差的问题。注册报告是消除出版偏见的一种方式，因为这种类型的科学文章在收集数据之前，会根据介绍、方法和统计分析计划进行审查[@chambers_past_2022;
@nosek_registered_2014]。经过该领域专家的同行评审，他们可能会对实验设计和数据分析方法提出改进意见。文章可以获准
"原则上接收"，这意味着只要遵循研究计划，无论结果如何，文章就会被发表。这应该有利于非显著性结果的发表，如图所示\@ref(fig:scheel)，对首次发表的心理学注册报告的分析显示，71篇文章中的31篇(44%)观察到了显著结果，相比之下，同期发表的152篇标准科学文章中的则有146篇(96%)观察到了显著结果[@scheel_excess_2021]
。

(ref:scheellab) 标准报告和注册报告的显著结果率，误差条表示95%置信区间。

```{r scheel, echo = FALSE, out.width  = '75%', fig.cap="(ref:scheellab)"}
knitr::include_graphics("images/scheel.png")
```

在过去，注册报告并不存在，科学家也不会分享所有的结果[@franco_publication_2014;
@greenwald_consequences_1975;
@sterling_publication_1959]，因此，我们必须努力检测出版偏差对我们准确评估文献能力的影响程度。元分析应该始终仔细检查出版偏差对元分析效应量估计的影响--尽管在1990年至2017年间发表在Psychological
Bulletin上的元分析中，估计只有57%文章说他们评估了出版偏差[@polanin_transparency_2020]。最近发表在教育研究上的元分析中，82%使用了偏差检测测试，但所使用的方法通常与最先进的方法相差甚远[@ropovik_neglect_2021]。目前已经开发了几种检验出版偏差的技术，但这仍然是一个非常活跃的研究领域。所有的技术都是基于特定的假设，你在应用检验测试之前应该考虑这些假设[@carter_correcting_2019]。目前并没有"妙方"：这些技术都不能纠正出版偏差。它们都不能肯定地告诉你修正了出版偏差后真正的元分析效应量是多少。这些方法能做到最好的事情就是在特定条件下检测由特定机制引起的出版偏差。出版偏差可以被检测出来，但它不能被修正。

在[似然](#likelihoods)一章中，我们看到混合结果是可以预期的，而且可以成为替代性假设的有力证据。不仅混合结果是可以预期的，而且可以专门观察到统计显著的结果，尤其是在统计检验力较低的情况下，这非常令人惊讶。以通常使用的80%的统计检验力下限，我们可以预期在有真实效果的情况下，五项研究中会有一项是不显著的结果。一些研究人员指出，在一组研究中，*不*发现混合结果非常不可能（换言之，"好得不真实"）[@francis_frequency_2014;
@schimmack_ironic_2012]。我们对真实的研究模式没有很好的感受，因为我们持续接触到的科学文献并不反映现实情况。科学文献中几乎所有的多项研究论文都只呈现统计显著的结果，尽管这不可能。

我们开发了计算二项式概率的[在线Shiny应用程序](http://shiny.ieis.tue.nl/mixed_results_likelihood/)。如果你滚动到页面底部，在"二项式概率"中找到"多项显著性发现"，给定一个关于检验力的特定假设，应用程序便可显示概率。@francis_frequency_2014使用这些二项式概率来计算2009年至2012年期间发表在Psychological
Science杂志上44篇包含四个或更多研究的文章的过度显著结果[@ioannidis_exploratory_2007]。他发现，对于这些文章中的36篇，考虑到根据观察到的效应量计算的平均检验力，观察到四个显著性结果的可能性小于10%。鉴于他选择的α水平为0.10，这个二项式概率是一个假设检验，并允许声称(在10%的α水平下)：只要统计显著的结果数量的二项式概率低于10%，数据是出乎意料的，由此我们可以拒绝"这是一组没有引入偏见的研究"这一假设。换句话说，不太可能观察到这么多显著的结果，这表明出版偏差或其他选择效应在这些文章中发挥了作用。

这44篇文章中，有一篇由我自己共同撰写[@jostmann_weight_2009]。那时，我对统计检验力和出版偏见知之甚少，被指责为进行不正当学术行为令我倍感压力。然而，这些指责是正确的--我们有选择地报告了结果，有选择地报告了有效的分析。由于几乎没有接受过这方面的培训，我们对自己进行了教育，并将一项未发表的研究上传到网站psychfiledrawer.org(该网站已不存在)，以分享我们的"文件抽屉"。若干年后，当Many
Labs
3将我们发表的一项研究纳入他们复制的研究集时，我们提供了帮助[@ebersole_many_2016]。当观察到一个无效的结果时，我们写道："我们不得不得出结论，实际上没有可靠的证据证明这种效应"[@jostmann_short_2016]。我希望这些教育材料能够避免其他人像我们一样出丑。

## Bias detection in meta-analysis
## 元分析中的偏差检测

检测发表偏差的新方法不断被开发出来，而旧的方法则变得过时(尽管你仍然可以看到它们出现在元分析中)。一种过时的方法被称为**故障安全N(fail-safe
N)**。其想法是计算在观察到的元分析效应量估计值不再与0有统计学差异之前，在文件抽屉中需要有多少个不显着的结果。这种方法已[不再被推荐](https://handbook-5-1.cochrane.org/chapter_10/10_4_4_3_fail_safe_n.htm)，Becker[-@becker_failsafe_2005]写道："鉴于现在有其他处理发表偏倚的方法，应该放弃故障安全N方法，而采用其他更富有信息量的分析方法。目前，故障安全N的唯一用途是作为一种工具来识别那些不是最先进的元分析。

在我们解释第二种方法(修剪和填充，Trim-and-Fill)之前，有必要解释一下可视化元分析的一种常见方法，即**漏斗图**。在漏斗图中，X轴用于绘制每个研究的效应量，Y轴用于绘制每个效应量的"精确度"(通常是每个效应量估计的标准误差)。一项研究中的观察数越多，效应量的估计就越精确，标准误差就越小，因此该研究在漏斗图中的位置就越高。一个无限精确的研究(标准误差为0)将位于y轴的顶端。

下面的代码模拟了`nsims`研究的荟萃分析，并存储了检查偏差检测所需的所有结果。在代码的第一部分中，模拟了所需方向上具有统计显着性的结果，在第二部分中生成了空结果。
如`pub.bias`所示，该代码会生成一定比例的重要结果------当设置为
11时，所有结果都是重要的。 在下面的代码中，`pub.bias`设置为 0.05。
因为模拟中没有真正的效果(`m1`和`m2`相等，所以组间没有差异)，唯一应该预期的显着结果是
5% 的误报。最后，执行元分析，输出结果，并创建漏斗图。

```{r metasim, eval = FALSE}
library(metafor)
library(truncnorm)

nsims <- 100 # number of simulated experiments
pub.bias <- 0.05 # set percentage of significant results in the literature

m1 <- 0 # too large effects will make non-significant results extremely rare
sd1 <- 1
m2 <- 0
sd2 <- 1
metadata.sig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                           n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)
metadata.nonsig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                              n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)

# simulate significant effects in the expected direction
if(pub.bias > 0){
for (i in 1:nsims*pub.bias) { # for each simulated experiment
  p <- 1 # reset p to 1
  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100)) # n based on truncated normal
  while (p > 0.025) { # continue simulating as along as p is not significant
    x <- rnorm(n = n, mean = m1, sd = sd1) 
    y <- rnorm(n = n, mean = m2, sd = sd2) 
    p <- t.test(x, y, alternative = "greater", var.equal = TRUE)$p.value
  }
  metadata.sig[i, 1] <- mean(x)
  metadata.sig[i, 2] <- mean(y)
  metadata.sig[i, 3] <- sd(x)
  metadata.sig[i, 4] <- sd(y)
  metadata.sig[i, 5] <- n
  metadata.sig[i, 6] <- n
  out <- t.test(x, y, var.equal = TRUE)
  metadata.sig[i, 7] <- out$p.value
  metadata.sig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
}}

# simulate non-significant effects (two-sided)
if(pub.bias < 1){
for (i in 1:nsims*(1-pub.bias)) { # for each simulated experiment
  p <- 0 # reset p to 1
  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100))
  while (p < 0.05) { # continue simulating as along as p is significant
    x <- rnorm(n = n, mean = m1, sd = sd1) # produce  simulated participants
    y <- rnorm(n = n, mean = m2, sd = sd2) # produce  simulated participants
    p <- t.test(x, y, var.equal = TRUE)$p.value
  }
  metadata.nonsig[i, 1] <- mean(x)
  metadata.nonsig[i, 2] <- mean(y)
  metadata.nonsig[i, 3] <- sd(x)
  metadata.nonsig[i, 4] <- sd(y)
  metadata.nonsig[i, 5] <- n
  metadata.nonsig[i, 6] <- n
  out <- t.test(x, y, var.equal = TRUE)
  metadata.nonsig[i, 7] <- out$p.value
  metadata.nonsig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
}}

# Combine significant and non-significant effects
metadata <- rbind(metadata.nonsig, metadata.sig)

# Use escalc to compute effect sizes
metadata <- escalc(n1i = n1, n2i = n2, m1i = m1, m2i = m2, sd1i = sd1, 
  sd2i = sd2, measure = "SMD", data = metadata[complete.cases(metadata),])
# add se for PET-PEESE analysis
metadata$sei <- sqrt(metadata$vi)

#Perform meta-analysis
result <- metafor::rma(yi, vi, data = metadata)
result

# Print a Funnel Plot
metafor::funnel(result, level = 0.95, refline = 0)
abline(v = result$b[1], lty = "dashed") # vertical line at meta-analytic ES
points(x = result$b[1], y = 0, cex = 1.5, pch = 17) # add point

```

让我们先看看无偏见的研究是什么样子的，运行代码，保持`pub.bias`在0.05，这样只有5%的第一类错误进入科学文献。

```{r metasim1, echo = FALSE}
library(metafor)
library(truncnorm)

set.seed(52)
nsims <- 100 # number of simulated experiments
pub.bias <- 0.05 # set percentage of significant results in the literature

m1 <- 0 # too large effects will make non-significant results extremely rare
sd1 <- 1
m2 <- 0
sd2 <- 1
metadata.sig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                           n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)
metadata.nonsig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                              n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)

# simulate significant effects in the expected direction
if(pub.bias > 0){
for (i in 1:nsims*pub.bias) { # for each simulated experiment
  p <- 1 # reset p to 1
  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100)) # n based on truncated normal
  while (p > 0.025) { # continue simulating as along as p is not significant
    x <- rnorm(n = n, mean = m1, sd = sd1) 
    y <- rnorm(n = n, mean = m2, sd = sd2) 
    p <- t.test(x, y, alternative = "greater", var.equal = TRUE)$p.value
  }
  metadata.sig[i, 1] <- mean(x)
  metadata.sig[i, 2] <- mean(y)
  metadata.sig[i, 3] <- sd(x)
  metadata.sig[i, 4] <- sd(y)
  metadata.sig[i, 5] <- n
  metadata.sig[i, 6] <- n
  out <- t.test(x, y, var.equal = TRUE)
  metadata.sig[i, 7] <- out$p.value
  metadata.sig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
}}

# simulate non-significant effects (two-sided)
if(pub.bias < 1){
for (i in 1:nsims*(1-pub.bias)) { # for each simulated experiment
  p <- 0 # reset p to 1
  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100))
  while (p < 0.05) { # continue simulating as along as p is significant
    x <- rnorm(n = n, mean = m1, sd = sd1) # produce simulated participants
    y <- rnorm(n = n, mean = m2, sd = sd2) # produce simulated participants
    p <- t.test(x, y, var.equal = TRUE)$p.value
  }
  metadata.nonsig[i, 1] <- mean(x)
  metadata.nonsig[i, 2] <- mean(y)
  metadata.nonsig[i, 3] <- sd(x)
  metadata.nonsig[i, 4] <- sd(y)
  metadata.nonsig[i, 5] <- n
  metadata.nonsig[i, 6] <- n
  out <- t.test(x, y, var.equal = TRUE)
  metadata.nonsig[i, 7] <- out$p.value
  metadata.nonsig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
}}

# Combine significant and non-significant effects
metadata <- rbind(metadata.nonsig, metadata.sig)

# Use escalc to compute effect sizes
metadata <- escalc(n1i = n1, n2i = n2, m1i = m1, m2i = m2, sd1i = sd1, 
  sd2i = sd2, measure = "SMD", data = metadata[complete.cases(metadata),])
# add se for PET-PEESE analysis
metadata$sei <- sqrt(metadata$vi)

#Perform meta-analysis
result <- metafor::rma(yi, vi, data = metadata)
result

```

当我们检查元分析的结果时，我们看到元分析中有100个研究(`k = 100`)，并且没有统计意义上的异质性(*p*
=
`r  round(result$QEp,2)`，这并不令人惊讶，因为我们在编程时将模拟的真实效应量设定为0，并且没有效应量的异质性)。我们也得到了元的结果。元分析的估计值是*d*=`r round(result$b[1],3)`，非常接近于0(应该如此，因为真实的效应量确实是0)。这个估计值的标准误差是`r round(result$se,3)`。有了100项研究，我们就有了对真实效应量的非常准确的估计。对*d*=0的检验的*Z*值是`r round(result$zval,3)`，而这个检验的*p*值是`r round(result$pval,2)`。我们不能拒绝真实效应量为0的假设。效应量估计值的置信区间(`r round(result$ci.lb,3)`,
`r round(result$ci.ub,3)`)包括0。

如果我们检查图中的漏斗图
\@ref(fig:funnel1)，我们看到每项研究都用一个点来表示。样本量越大，在图中越高，样本量越小，在图中越低。白色的金字塔代表一项研究不具有统计学显著性的区域，因为观察到的效应量(X轴)与0相差不大，以至于观察到的效应量周围的置信区间会排除0。标准误差越小，置信区间就越窄，效应量就越小，以达到统计学上的意义。同时，标准误差越小，效应量就越接近真实的效应量，所以我们就越不可能看到远离0的效应。我们看到只有少数研究(确切地说，是五项)落在图的右侧的白色金字塔之外。这些是我们在模拟中编排的5%的重要结果。请注意，这5项研究都是假阳性，因为没有真正的效果。如果有真正的影响(你可以重新运行模拟，通过将模拟中的`m1 <- 0`改为`m1 <- 0.5`，将*d*设置为0.5)，金字塔云的点会向右移动，并以0.5而不是0为中心。

(ref:funnel1lab) 无偏见的无效结果漏斗图。

```{r funnel1, echo = FALSE, fig.cap="(ref:funnel1lab)"}
# Print a Funnel Plot
par(bg = backgroundcolor)
metafor::funnel(result, level = 0.95, refline = 0)
abline(v = result$b[1], lty = "dashed") #  vertical line at meta-analytic ES
points(x = result$b[1], y = 0, cex = 1.5, pch = 17) # add point

```

我们现在可以将上面的无偏元分析与有偏差的元分析进行比较。
我们可以模拟具有极端出版偏差的情况。
[基于\@scheel](mailto:基于@scheel){.email}\_excess_2021 的估计，我们假设
96% 的研究显示出阳性结果。 我们在代码中设置`pub.bias <- 0.96`。
我们将两个均值都保持为
0，因此仍然没有真正的效果，但在最后一组研究中，我们最终会在预测方向上主要出现第一类错误。在模拟出有偏差的结果后，我们可以进行元分析，看看基于元分析的统计推断是否具有误导性。

```{r metasim2, echo = FALSE}
set.seed(5)
nsims <- 100 # number of simulated experiments
pub.bias <- 0.96 # set percentage of significant results in the literature

m1 <- 0 # too large effects will make non-significant results extremely rare
sd1 <- 1
m2 <- 0
sd2 <- 1
metadata.sig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                           n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)
metadata.nonsig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                              n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)

# simulate significant effects in the expected direction
if(pub.bias > 0){
for (i in 1:nsims*pub.bias) { # for each simulated experiment
  p <- 1 # reset p to 1
  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100)) # n based on truncated normal
  while (p > 0.025) { # continue simulating as along as p is not significant
    x <- rnorm(n = n, mean = m1, sd = sd1) 
    y <- rnorm(n = n, mean = m2, sd = sd2) 
    p <- t.test(x, y, alternative = "greater", var.equal = TRUE)$p.value
  }
  metadata.sig[i, 1] <- mean(x)
  metadata.sig[i, 2] <- mean(y)
  metadata.sig[i, 3] <- sd(x)
  metadata.sig[i, 4] <- sd(y)
  metadata.sig[i, 5] <- n
  metadata.sig[i, 6] <- n
  out <- t.test(x, y, var.equal = TRUE)
  metadata.sig[i, 7] <- out$p.value
  metadata.sig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
}}

# simulate non-significant effects (two-sided)
if(pub.bias < 1){
for (i in 1:nsims*(1-pub.bias)) { # for each simulated experiment
  p <- 0 # reset p to 1
  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100))
  while (p < 0.05) { # continue simulating as along as p is significant
    x <- rnorm(n = n, mean = m1, sd = sd1) # produce  simulated participants
    y <- rnorm(n = n, mean = m2, sd = sd2) # produce  simulated participants
    p <- t.test(x, y, var.equal = TRUE)$p.value
  }
  metadata.nonsig[i, 1] <- mean(x)
  metadata.nonsig[i, 2] <- mean(y)
  metadata.nonsig[i, 3] <- sd(x)
  metadata.nonsig[i, 4] <- sd(y)
  metadata.nonsig[i, 5] <- n
  metadata.nonsig[i, 6] <- n
  out <- t.test(x, y, var.equal = TRUE)
  metadata.nonsig[i, 7] <- out$p.value
  metadata.nonsig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
}}

# Combine significant and non-significant effects
metadata <- rbind(metadata.nonsig[complete.cases(metadata.nonsig),], metadata.sig[complete.cases(metadata.sig),])

# Use escalc to compute effect sizes
metadata <- metafor::escalc(n1i = n1, n2i = n2, m1i = m1, m2i = m2, sd1i = sd1, 
  sd2i = sd2, measure = "SMD", data = metadata)
# add se for PET-PEESE analysis
metadata$sei <- sqrt(metadata$vi)

#Perform meta-analysis
result.biased <- metafor::rma(yi, vi, data = metadata)
result.biased

```

如果我们研究一下图中的漏斗图\@ref(fig:funnel2)，就会发现我们所分析的这组研究的偏颇性。这个模式非常奇特。我们看到有四项无偏差的无效结果，正如我们在模拟中所设定的那样，但96项研究中的其余部分都在统计学上显著，尽管实际上并没有效应。我们看到大多数研究正好落在白色金字塔的边缘。由于*p*值在空值下是均匀分布的，我们观察到的第一类错误的*p*值往往在0.02到0.05之间，这与我们在存在真实效应的情况下的预期不同。这些仅仅是有意义的*p*值刚好落在白色金字塔的外面。研究规模越大，有意义的效应量就越小。事实上，效应量并不围绕单一的真实效应量而变化(例如，*d*=0或*d*=0.5)，而是效应量随着样本量的增大(或标准误差的减小)而变小，这是一个强有力的偏差指标。图中顶部的垂直虚线和黑色三角形说明了观察到的(向上偏差的)元分析效应量估计。

(ref:funnel2lab) 有偏见的无效结果的漏斗图，大部分是显著的结果。

```{r funnel2, echo = FALSE, fig.cap="(ref:funnel2lab)"}
# Print a Funnel Plot
par(bg = backgroundcolor)
metafor::funnel(result.biased, level = 0.95, refline = 0)
abline(v = result.biased$b[1], lty = "dashed") #  vertical line at meta-analytic ES
points(x = result.biased$b[1], y = 0, cex = 1.5, pch = 17) # add point
```

人们可能会想，在科学研究中是否真的出现过这种极端的偏见。确实如此。在图\@ref(fig:carterbias)中，[我们看到了\@carter](mailto:我们看到了@carter){.email}\_publication_2014的漏斗图，他研究了198项测试"自我消耗
"效应的已发表研究中的偏见，即自我控制依赖于有限资源的观点。你是否注意到与我们上面模拟的极度有偏见的元分析有任何相似之处？你可能不会感到惊讶，即使在2015年之前，研究人员认为有大量可靠的文献证明了自我消耗效应，但一份预注册的复制报告得出的效果大小估计并不显著[@hagger_multilab_2016]，甚至当原始研究人员试图复制自己的工作时，他们也未能观察到自我消耗的显著效果[@vohs_multisite_2021]。想象一下，在一篇完全基于科学研究偏见的文献上，浪费了大量的时间、精力和金钱。显然，这种研究浪费具有伦理意义，研究人员需要负起责任，防止未来出现这种浪费。

(ref:carterbiaslab)Carter和McCullough(2014)的漏斗图，显示198个已发表的自我消耗效应测试中的偏差。

```{r carterbias, echo = FALSE, fig.cap="(ref:carterbiaslab)"}
knitr::include_graphics("images/carterfunnel.png")
```

我们也可以在元分析的森林图中看到偏见的迹象。在图\@ref(fig:twoforestplot)中，两个森林图是并排绘制的。左边的森林图是基于无偏见的数据，右边的森林图是基于有偏见的数据。100个研究的森林图有点大，但我们看到，在左边的森林图中，效果随机地在0附近变化，它们本该如此。在右边，在前四项研究之外，所有的置信区间神奇地排除了0的效应。

(ref:twoforestplotlab) 无偏见元分析(左)和有偏见元分析(右)的森林图。

```{r twoforestplot, echo = FALSE, fig.width=5, fig.height=15, fig.cap="(ref:twoforestplotlab)"}
par(mfrow=c(1,2))
par(mar=c(5,4,1,1))
par(bg = backgroundcolor)
forest(result, annotate=FALSE, cex=.8, at=seq(-2,2,1), digits=1, xlim=c(-5,2))
par(mar=c(5,3,1,2))
forest(result.biased, annotate=FALSE, slab=rep("",length(result$yi)), cex=.8, at=seq(-2,2,1), digits=1, xlim=c(-4,3))
par(mfrow=c(1,1))
```

当研究人员只发表有统计学意义的结果(*p* \<
$\alpha$)而存在发表偏差时，你在元分析中计算效应量，与没有发表偏差时相比，存在发表偏差时(研究人员只发表*p*\<
$alpha$的效应)的元分析效应量估计值会**偏高**。这是因为出版偏差过滤掉了较小的(不显著的)效应量，然后不包括在元分析效应量的计算中。这导致元分析效应量的估计值大于真实效应量。在强烈的发表偏差下，我们知道元分析效应量被夸大了，但我们不知道有多大。真正的效应量可能只是小一点，但真正的效应量也可能是0，例如在自我耗损相关文献的情况下。


## Trim and Fill
## 削减和填充 

Trim and fill is a technique that aims to augment a dataset by adding hypothetical ‘missing’ studies (that may be in the ‘file-drawer’). The procedure starts by removing (‘trimming’) small studies that bias the meta-analytic effect size, then estimates the true effect size, and ends with ‘filling’ in a funnel plot with studies that are assumed to be missing due to publication bias. In the Figure \@ref(fig:trimfill1), you can see the same funnel plot as above, but now with added hypothetical studies (the unfilled circles which represent ‘imputed’ studies). If you look closely, you’ll see these points each have a mirror image on the opposite side of the meta-analytic effect size estimate (this is clearest in the lower half of the funnel plot). If we examine the result of the meta-analysis that includes these imputed studies, we see that trim and fill successfully alerts us to the fact that the meta-analysis is biased (if not, it would not add imputed studies) but it fails miserably in correcting the effect size estimate. In the funnel plot, we see the original (biased) effect size estimate indicated by the triangle, and the meta-analytic effect size estimate adjusted with the trim-and-fill method (indicated by the black circle). We see the meta-analytic effect size estimate is a bit lower, but given that the true effect size in the simulation was 0, the adjustment is clearly not sufficient.  
削减和填充是一种技术，旨在通过添加假设的“缺失”研究（可能在“文件抽屉”中）来增加数据集。该程序首先通过删除（“削减”）会对荟萃分析效应量造成偏差的小型研究，然后估计真实的效应量，并最终使用假设的缺失研究（由于出版偏差而缺失）填充漏斗图。在图\@ref(fig:trimfill1)中，您可以看到与上述相同的漏斗图，但现在添加了假设的研究（未填充的圆点表示“插补”研究）。如果您仔细观察，您会发现每个点都在荟萃分析效应量估计的相反侧具有镜像图像（这在漏斗图的下半部分最清晰）。如果我们检查包括这些插补研究的荟萃分析结果，我们会发现削减和填充成功地提醒我们该荟萃分析存在偏差（如果没有，它不会添加插补研究），但它在校正效应量估计方面失败了。在漏斗图中，我们看到由三角形表示的原始（有偏）效应量估计和使用削减和填充方法调整的荟萃分析效应量估计（由黑色圆圈表示）。我们看到荟萃分析效应量估计略微降低，但考虑到模拟中真实的效应量为0，此调整显然不足。



(ref:trimfill1lab) 通过削减和填充添加了假定缺失效果的漏斗图.

```{r trimfill1, fig.margin = FALSE, echo = FALSE, fig.cap="(ref:trimfill1lab)"}

result.trimfill <- metafor::trimfill(result.biased)
par(bg = backgroundcolor)
metafor::funnel(result.trimfill, refline = 0)
abline(v = result.biased$b[1], lty = "dashed") #  vertical line at meta-analytic ES
points(x = result.biased$b[1], y = 0, cex = 1.5, pch = 17) # add point
abline(v = result.trimfill$b[1], lty = "dotted") #  vertical line at meta-analytic ES
points(x = result.trimfill$b[1], y = 0, cex = 1.5, pch = 16) # add point
```

Trim-and-fill is not very good under many realistic publication bias scenarios. The method is criticized for its reliance on the strong assumption of symmetry in the funnel plot. When publication bias is based on the *p*-value of the study (arguably the most important source of publication bias in many fields) the trim-and-fill method does not perform well enough to yield a corrected meta-analytic effect size estimate that is close to the true effect size [@peters_performance_2007; @terrin_adjusting_2003]. When the assumptions are met, it can be used as a **sensitivity analysis.** Researchers should not report the trim-and-fill corrected effect size estimate as a realistic estimate of the unbiased effect size. If other bias-detection tests (like *p*-curve or *z*-curve discussed below) have already indicated the presence of bias, the trim-and-fill procedure might not provide additional insights. 

削减和填充在许多实际出版偏差场景下表现不佳。该方法因依赖漏斗图对称性的强假设而备受批评。当出版偏差基于研究的*p*值时（在许多领域中可能是最重要的出版偏差来源），削减和填充方法的效果不足以产生接近真实效应大小的纠正荟萃分析效应量估计[@peters_performance_2007; @terrin_adjusting_2003]。在假设成立的情况下，它可以作为**敏感性分析**的一种方法。研究人员不应将削减和填充校正的效应量估计报告为无偏效应量的实际估计。如果其他偏差检测测试（如下文所述的*p*曲线或*z*曲线）已经指示存在偏差，则削减和填充程序可能无法提供额外的洞见。


## PET-PEESE  
## 精度效应测试-带标准误差的精度效应估计
A novel class of solutions to publication bias is **meta-regression**. Instead of plotting a line through individual data-points, in meta-regression a line is plotted through data points that each represent a study. As with normal regression, the more data meta-regression is based on, the more precise the estimate is and, therefore, the more studies in a meta-analysis, the better meta-regression will work in practice. If the number of studies is small, all bias detection tests lose power, and this is something that one should keep in mind when using meta-regression. Furthermore, regression requires sufficient variation in the data, which in the case of meta-regression means a wide range of sample sizes (recommendations indicate meta-regression performs well if studies have a range from 15 to 200 participants in each group – which is not typical for most research areas in psychology). Meta-regression techniques try to estimate the population effect size if precision was perfect (so when the standard error = 0).  
一种新的解决出版偏差的方法是**荟萃回归**。荟萃回归不是通过描绘数据点的线条来展示数据，而是通过描绘每个代表一项研究的数据点的线条来呈现数据。与普通的回归分析类似，荟萃回归基于的数据越多，估计结果越精确，因此，在进行荟萃分析时，研究数量越多，荟萃回归在实践中的效果就会越好。如果研究数量很少，所有偏差检测测试都会失去功效，使用荟萃回归时需要记住这一点。此外，回归分析需要数据的足够变异性，而在荟萃回归中，这意味着需要广泛的样本量范围（建议指出，如果每组研究对象的范围在15至200人之间，荟萃回归方法的效果会更好——这在心理学的大多数研究领域并不典型）。荟萃回归技术试图估计在精度完美时（标准误差 = 0）的总体效应大小。

One meta-regression technique is known as PET-PEESE [@stanley_meta-regression_2014; @stanley_finding_2017]. It consists of a ‘precision-effect-test’ (PET) which can be used in a Neyman-Pearson hypothesis testing framework to test whether the meta-regression estimate can reject an effect size of 0 based on the 95% CI around the PET estimate at the intercept SE = 0. Note that when the confidence interval is very wide due to a small number of observations, this test might have low power, and have an a-priori low probability of rejecting the null effect. The estimated effect size for PET is calculated with: $d = β_0 + β_1SE_i + _ui$ where d is the estimated effect size, SE is the standard error, and the equation is estimated using weighted least squares (WLS), with 1/SE2i as the weights. The PET estimate underestimates the effect size when there is a true effect. Therefore, the PET-PEESE procedure recommends first using PET to test whether the null can be rejected, and if so, then the  'precision-effect estimate with standard error' (PEESE) should be used to estimate the meta-analytic effect size. In PEESE, the standard error (used in PET) is replaced by the variance (i.e., the standard error squared), which @stanley_meta-regression_2014 find reduces the bias of the estimated meta-regression intercept.  

一种荟萃回归技术被称为PET-PEESE[@stanley_meta-regression_2014; @stanley_finding_2017]。它由一个“精度效应测试”（PET）组成，可以在Neyman-Pearson假设检验框架中使用，以检验荟萃回归估计能否基于PET估计的95%置信区间拒绝效应量为0的假设，即在截距SE=0处。需要注意的是，当由于观测值很少而置信区间非常宽时，这个测试可能具有低功效，并且先验地低概率拒绝零效应。通过以下公式计算PET的估计效应量：$d = β_0 + β_1SE_i + _ui$，其中d为估计效应量，SE为标准误差，该方程式使用加权最小二乘（WLS）估计，并以1/SE^2i作为权重。PET估计低估了真实效应大小。因此，PET-PEESE程序建议首先使用PET检验能否拒绝零假设，如果可以，则使用“带标准误差的精度效应估计”（PEESE）来估计荟萃分析效应量。在PEESE中，标准误差（在PET中使用）被方差（即标准误差的平方）所代替，@stanley_meta-regression_2014发现这可以减少荟萃回归截距的估计偏差。


PET-PEESE has limitations, as all bias detection techniques have. The biggest limitations are that it does not work well when there are few studies, all the studies in a meta-analysis have small sample sizes, or when there is large heterogeneity in the meta-analysis [@stanley_finding_2017]. When these situations apply (and they will in practice), PET-PEESE might not be a good approach. Furthermore, there are some situations where there might be a correlation between sample size and precision, which in practice will often be linked to heterogeneity in the effect sizes included in a meta-analysis. For example, if true effects are different across studies, and people perform power analyses with accurate information about the expected true effect size, large effect sizes in a meta-analysis will have small sample sizes, and small effects will have large sample sizes. Meta-regression is, like normal regression, a way to test for an association, but you need to think about the causal mechanism behind the association.
PET-PEESE也存在局限性，就像所有的偏差检测技术一样。最大的限制是，当研究数量很少，荟萃分析中的所有研究样本量都很小，或者当荟萃分析中存在大的异质性[@stanley_finding_2017]时，它的效果不佳。在这些情况下（实际上这些情况很常见），PET-PEESE可能不是一个好的选择。此外，有些情况下样本量和精度可能存在相关性，这在实践中通常与荟萃分析中包含的效应大小的异质性有关。例如，如果不同的研究具有不同的真实效应，而人们使用有关预期真实效应大小的准确信息进行功效分析，则荟萃分析中的大效应量将具有较小的样本量，而小效应量则具有较大的样本量。荟萃回归和普通的回归分析一样，是一种测试关联性的方法，但你需要思考关联性背后的因果机制。

Let’s explore how PET-PEESE meta-regression attempts to give us an unbiased effect size estimate, under specific assumptions of how publication bias is caused. In Figure \@ref(fig:petpeese) we once again see the funnel plot, now complemented with 2 additional lines through the plots. The vertical line at *d* = 0.27 is the meta-analytic effect size estimate, which is upwardly biased because we are averaging over statistically significant studies only. There are 2 additional lines, which are the meta-regression lines for PET-PEESE based on the formulas detailed previously. The straight diagonal line gives us the PET estimate at a SE of 0 (an infinite sample, at the top of the plot), indicated by the circle. The dotted line around this PET estimate is the 95% confidence interval for the estimate. In this case, the 95% CI contains 0, which means that based on the PET estimate of *d* = 0.02, we cannot reject a meta-analytic effect size of 0. Note that even with 100 studies, the 95% CI is quite wide. Meta-regression is, just like normal regression, only as accurate as the data we have. This is one limitation of PET-PEESE meta-regression: With small numbers of studies in the meta-analysis, it has low accuracy. If we had been able to reject the null based on the PET estimate, we would then have used the PEESE estimate (indicated by the diamond shap) of *d* = 0.17 for the meta-analytic effect size, corrected for bias (while never knowing whether the model underlying the PEESE estimate corresponded to the true bias generating mechanisms in the meta-analysis, and thus if the meta-analytic estimate was accurate).  
让我们探讨一下PET-PEESE荟萃回归是如何在特定假设出版偏差的情况下尝试给出无偏效应量估计的。在图\@ref(fig:petpeese)中，我们再次看到漏斗图，现在还有两条额外的线穿过图表。竖直线在*d* = 0.27处是荟萃分析效应量估计，因为我们只对显著性研究进行平均，所以这个估计值会向上偏差。有两条附加的线，它们是基于先前详细介绍的公式，PET-PEESE荟萃回归的元回归线。直线对角线给出了在SE为0（即在图的顶部，具有无限样本）的PET估计，由圆圈表示。围绕这个PET估计的虚线是估计的95%置信区间。在这种情况下，95%置信区间包含0，这意味着基于*d* = 0.02的PET估计，我们不能拒绝荟萃分析效应量为0的假设。请注意，即使有100项研究，95%置信区间也很宽。荟萃回归和普通的回归分析一样，只有我们拥有的数据准确性才能得到保证。这是PET-PEESE荟萃回归的一个局限性：在荟萃分析中的研究数量较少时，它的准确性较低。如果我们能够基于PET估计拒绝零假设，那么我们将使用PEESE估计（由菱形形状表示）的*d* = 0.17进行纠正偏差的荟萃分析效应量估计（同时永远不知道PEESE估计模型是否与荟萃分析中真实的偏差产生机制相对应，因此无法确定荟萃分析估计值是否准确）。
(ref:petpeeselab) 带有PETPEESE回归线的漏斗图。

```{r petpeese, echo = FALSE, fig.cap="(ref:petpeeselab)"}

# PET PEESE code below is adapted from Joe Hilgard: https://github.com/Joe-Hilgard/PETPEESE/blob/master/PETPEESE_functions.R
# PET
PET <- metafor::rma(yi = yi, sei = sei, mods = ~sei, data = metadata, method = "FE")

# PEESE
PEESE <- metafor::rma(yi = yi, sei = sei, mods = ~I(sei^2), data = metadata, method = "FE")

# Funnel Plot 
par(bg = backgroundcolor)
metafor::funnel(result.biased, level = 0.95, refline = 0, main = paste("FE d =", round(result.biased$b[1],2),"PET d =", round(PET$b[1],2),"PEESE d =", round(PEESE$b[1],2)))
abline(v = result.biased$b[1], lty = "dashed") #draw vertical line at meta-analytic effect size estimate
points(x = result.biased$b[1], y = 0, cex = 1.5, pch = 17) #draw point at meta-analytic effect size estimate
# PET PEESE code below is adapted from Joe Hilgard: https://github.com/Joe-Hilgard/PETPEESE/blob/master/PETPEESE_functions.R
# PEESE line and point
sei <- (seq(0, max(sqrt(result.biased$vi)), .001))
vi <- sei^2
yi <- PEESE$b[1] + PEESE$b[2]*vi
grid <- data.frame(yi, vi, sei)
lines(x = grid$yi, y = grid$sei, typ = 'l') # add line for PEESE
points(x = (PEESE$b[1]), y = 0, cex = 1.5, pch = 5) # add point estimate for PEESE
# PET line and point
abline(a = -PET$b[1]/PET$b[2], b = 1/PET$b[2]) # add line for PET
points(x = PET$b[1], y = 0, cex = 1.5) # add point estimate for PET
segments(x0 = PET$ci.lb[1], y0 = 0, x1 = PET$ci.ub[1], y1 = 0, lty = "dashed") #Add 95% CI around PET

```

## *P*-value meta-analysis  
## *p*值的荟萃分析
In addition to a meta-analysis of effect sizes, it is possible to perform a meta-analysis of *p*-values. The first of such approaches is known as the [**Fisher's combined probability test**](https://en.wikipedia.org/wiki/Fisher%27s_method), and more recent bias detection tests such as *p*-curve analysis [@simonsohn_p-curve_2014] and *p*-uniform* [@aert_correcting_2018] build on this idea. These two techniques are an example of selection model approaches to test and adjust for meta-analysis [@iyengar_selection_1988], where a *model about the data generating process* of the effect sizes is combined with a *selection model* of how publication bias impacts which effect sizes become part of the scientific literature. An example of a data generating process would be that results of studies are generated by statistical tests where all test assumptions are met, and the studies have some average power. A selection model might be that all studies are published, as long as they are statistically significant at an alpha level of 0.05. 
除了效应量的荟萃分析外，还可以进行*p*值的荟萃分析。这种方法中的第一个是[**Fisher综合概率检验**](https://en.wikipedia.org/wiki/Fisher%27s_method)，而更近期的偏差检测测试，如*p*曲线分析[@simonsohn_p-curve_2014]和p*p*均匀分布[@aert_correcting_2018]则是基于此思想。这两种技术是选择模型方法的例子，用于测试和调整荟萃分析[@iyengar_selection_1988]，其中将关于效应量*数据生成过程的模型*与关于发表偏差如何影响哪些效应量成为科学文献一部分的*选择模型*相结合。数据生成过程的一个示例是，研究结果是由满足所有检验假设的统计检验生成的，并且这些研究具有一定的平均功效。选择模型可能是只要研究在0.05的显著性水平下具有统计显著性，就会发表所有的研究。

*P*-curve analysis uses exactly this selection model. It assumes all significant results are published, and examines whether the data generating process mirrors what would be expected if the studies have a certain power, or whether the data generating process mirrors the pattern expected if the null hypothesis is true. As discussed in the section on [which *p*-values you can expect](#whichpexpect) we should observe uniformly distributed *p*-values when the null hypothesis is true, and more small significant *p*-values (e.g., 0.01) than large significant *p*-values (e.g., 0.04) when the alternative hypothesis is true. *P*-curve analysis performs two tests. In the first test, *p*-curve analysis examines whether the *p*-value distribution is flatter than what would be expected if the studies you analyze had 33% power. This value is somewhat arbitrary (and can be adjusted), but the idea is to reject at the smallest level of statistical power that would lead to useful insights about the presence of effects. If the average power in the set of studies is less than 33%, there might be an effect, but the studies are not designed well enough to learn about it by performing statistical tests. If we can reject the presence of a pattern of *p*-values that has at least 33% power, this suggests the distribution looks more like one expected when the null hypothesis is true. That is, we would doubt there is an effect in the set of studies included in the meta-analysis, *even though all individual studies were statistically significant*.

*P*曲线分析正是利用了这个选择模型。它假设所有显著结果都被发表，并检查数据生成过程是否符合研究具有一定功效时所期望的模式，或者数据生成过程是否符合零假设成立时所期望的模式。正如在你可以期望哪些p值的部分中所讨论的那样，当零假设成立时，我们应该观察到p值均匀分布，而当备择假设成立时，我们应该观察到比大的显著*p*值（例如0.04）更多的小的显著*p*值（例如0.01）。*p*值曲线分析进行两个测试。在第一个测试中，*p*值曲线分析检查*p*值分布是否比你分析的研究具有33%的功效时所期望的分布更为平坦。这个值有点武断（但可以调整），但其想法是在最小的统计功效水平上拒绝，这将导致有关存在效应的有用见解。如果研究集中的平均功效小于33%，可能存在效应，但这些研究的设计不足以通过统计检验来学习效应。如果我们可以拒绝至少具有33%功效的p值模式的存在，这表明分布更像是零假设成立时所期望的。也就是说，即使所有单独的研究都是显著的，我们也会怀疑荟萃分析所包含的研究集中是否存在效应。

The second test examines whether the *p*-value distribution is sufficiently right-skewed (more small significant *p*-values than large significant *p*-values), such that the pattern suggests we can reject a uniform *p*-value distribution. If we can reject a uniform *p*-value distribution, this suggests the studies might have examined a true effect and had at least some power. If the second test is significant, we would act as if the set of studies examines some true effect, even though there might be publication bias. As an example, let’s consider Figure 3 from Simonsohn and colleagues [-@simonsohn_p-curve_2014]. The authors compared 20 papers in the Journal of Personality and Social Psychology that used a covariate in the analysis, and 20 studies that did not use a covariate. The authors suspected that researchers might add a covariate in their analyses to try to find a *p*-value smaller than 0.05, when the first analysis they tried did not yield a significant effect.  
第二个测试检查*p*值分布是否足够右偏倚（更多的小显著*p*值而非大显著*p*值），以便这种模式表明我们可以拒绝均匀的p-值分布。如果我们可以拒绝具有至少33％功率的p-值模式的存在，这表明分布看起来更像是空假设为真时预期的分布。也就是说，我们会怀疑该元分析中包含的研究是否存在效应，即使所有独立的研究都是统计显著的。第二个测试检验是否存在足够的右偏，以拒绝均匀分布的*p*值分布。如果第二个测试是显著的，我们将视为这些研究检验了某些真正的效应，即使存在出版偏差。例如，让我们考虑Simonsohn和同事[-@simonsohn_p-curve_2014]的图3。作者比较了20篇在《人格与社会心理学杂志》(Journal of Personality and Social Psychology)中使用协变量的论文和20个未使用协变量的研究。作者怀疑研究人员可能在分析中添加协变量，以尝试找到小于0.05的*p*值，当他们第一次尝试的分析没有产生显著的效应时。


(ref:pcurvelab) Simonsohn等人(2014)的图3显示了有偏和无偏的*p*值曲线。

```{r pcurve, fig.margin = FALSE, echo = FALSE, fig.cap="(ref:pcurvelab)"}
knitr::include_graphics("images/pcurve.png")
```

The *p*-curve distribution of the observed *p*-values is represented by five points in the blue line. *P*-curve analysis is performed *only* on statistically significant results, based on the assumption that these are always published, and thus that this part of the *p*-value distribution contains all studies that were performed. The 5 points illustrate the percentage of *p*-values between 0 and 0.01, 0.01 and 0.02, 0.02 and 0.03, 0.03 and 0.04, and 0.04 and 0.05. In the figure on the right, you see a relatively normal right-skewed *p*-value distribution, with more low than high *p*-values. The *p*-curve analysis shows that the blue line in the right figure is more right-skewed than the uniform red line (where the red line is the uniform *p*-value distribution expected if there was no effect). Simonsohn and colleagues summarize this pattern as an indication that the set of studies has 'evidential value', but this terminology is somewhat misleading. The formally correct interpretation is that we can reject a *p*-value distribution as expected when the null hypothesis was true in all studies included in the *p*-curve analysis. Rejecting a uniform *p*-value distribution does not automatically mean there is evidence for the theorized effect (e.g., the pattern could be caused by a mix of null effects and a small subset of studies that show an effect due to a methodological confound). 
观察到的*p*值的*p*值曲线分布由蓝线上的五个点表示。*p*值曲线分析仅对统计显著的结果进行，基于这样的假设，即这些结果总是被发表，因此这部分*p*值值分布包含了所有已进行的研究。这五个点表示了*p*值值在0到0.01、0.01到0.02、0.02到0.03、0.03到0.04和0.04到0.05之间的百分比。在右图中，可以看到一个相对正常的右偏的*p*值分布，其中低*p*值比高*p*值多。*P*值曲线分析显示，右图中的蓝线比均匀的红线更右偏（其中红线是预期如果没有效应的情况下的均匀*p*值分布）。Simonsohn和同事将这种模式概括为一组研究具有“证据价值”，但这种术语有些误导。正确的解释是我们可以拒绝均匀的*p*值分布，这是在*p*值曲线分析中包括的所有研究中当零假设为真时预期的情况。拒绝均匀的*p*值值分布并不意味着自动存在对理论效应的证据（例如，该模式可能由一组空效应和少数显示效应的研究组成，这是由于方法上的混杂因素引起的）。

In the left figure we see the opposite pattern, with mainly high *p*-values around 0.05, and almost no *p*-values around 0.01. Because the blue line is significantly flatter than the green line, the *p*-curve analysis suggests this set of studies is the result of selection bias and was not generated by a set of sufficiently powered studies. *P*-curve analysis is a useful tool. But it is important to correctly interpret what a *p*-curve analysis can tell you. A right-skewed *p*-curve does not prove that there is no bias, or that the theoretical hypothesis is true. A flat *p*-curve does not prove that the theory is incorrect, but it does show that the studies that were meta-analyzed look more like the pattern that would be expected if the null hypothesis was true, and there was selection bias.  
在左图中，我们看到相反的模式，主要是高*p*值（约0.05），几乎没有*p*值在0.01左右。由于蓝线显著平坦，比绿线更平坦，*p*值曲线分析表明这些研究集合是受到选择性偏差的影响，并且不是由一组具有足够功效的研究产生的。*P*值曲线分析是一个有用的工具，但正确解释它能告诉你什么是很重要的。右偏的*p*值曲线不能证明不存在偏差或理论假设成立。平坦的*p*值曲线不能证明理论不正确，但它表明被元分析的研究看起来更像是在零假设成立且存在选择性偏差的情况下期望的模式。


该脚本存储元分析中包含的100个模拟*t*测试的所有测试统计信息。前几行如下所示：
```{r pcurveinput, echo = FALSE}
cat(metadata$pcurve[1:5],sep = "\n")
```

Print all test results with `cat(metadata$pcurve, sep = "\n")`, and go to the online *p*-curve app at  <http://www.p-curve.com/app4/>. Paste all the test results, and click the ‘Make the p-curve’ button. Note that the *p*-curve app will only yield a result when there are *p*-values smaller than 0.05 - if all test statistics yield a *p* \> 0.05, the *p*-curve cannot be computed, as these tests are ignored.
请使用`cat(metadata$pcurve, sep = "\n")`打印出所有测试结果，并转到在线*p*值曲线应用程序<http://www.p-curve.com/app4/>。粘贴所有测试结果，然后单击“生成*p*值曲线”按钮。请注意，*p*值曲线应用程序仅在存在小于0.05的*p*值时才会产生结果，如果所有测试统计量的*p* > 0.05，则无法计算*p*值曲线，因为这些测试将被忽略。

(ref:pcurveresultlab) 偏倚研究的*p*值曲线分析结果。

```{r pcurveresult, fig.margin = FALSE, echo = FALSE, fig.cap="(ref:pcurveresultlab)"}
knitr::include_graphics("images/pcurveresult.png")
```

The distribution of *p*-values clearly looks like it comes from a uniform distribution (as it indeed does), and the statistical test indicates we can reject a *p*-value distribution as steep or steeper as would be generated by a set of studies with 33% power, *p* < 0.0001. The app also provides an estimate of the average power of the tests that generated the observed *p*-value distribution, 5%, which is indeed correct. Therefore, we can conclude these studies, even though many effects are statistically significant, are more in line with selective reporting of Type 1 errors, than with a *p*-value distribution that should be expected if there was a true effect that was studied with sufficient statistical power. The theory might still be true, but the set of studies we have analyzed here do not provide support for the theory.
*P*值的分布明显呈均匀分布（事实上就是如此），而统计检验表明我们可以拒绝一个*p*值分布与33%功效水平的研究生成的*p*值分布一样陡峭或更陡峭的假设，*p* < 0.0001。该应用程序还提供了生成观察到的*p*值分布的测试的平均功效估计，为5%，这是正确的。因此，我们可以得出结论，即这些研究，即使许多效应在统计上是显著的，更符合选择性报告第一类错误的类型，而不是符合应该期望存在足够的统计功效研究的真实效应的*p*值分布。该理论仍然可能是正确的，但我们在这里分析的研究并不支持该理论。

A similar meta-analytic technique is *p*-uniform\*. This technique is similar to *p*-curve analysis and selection bias models, but it uses the results both from significant and non-significant studies, and can be used to estimate a bias-adjusted meta-analytic effect size estimate. The technique uses a random-effects model to estimate the effect sizes for each study, and weighs them based on a selection model that assumes significant results are more likely to be published than non-significant results. Below, we see the output of the *p*-uniform\* which estimates the bias-corrected effect size to be *d* = 0.0126. This effect size is not statistically different from 0, *p* = 0.3857, and therefore this bias detection technique correctly indicates that even though all effects were statistically significant, the set of studies does not provide a good reason to reject a meta-analytic effect size estimate of 0.
类似的元分析技术是*p-uniform*。这种技术类似于*p*值曲线分析和选择性偏差模型，但它使用了显著和非显著研究的结果，并可用于估计校正偏差的元分析效应量估计。该技术使用随机效应模型估计每个研究的效应量，并根据一个选择模型对它们进行加权，该模型假设显著结果比非显著结果更有可能被发表。下面是*p-uniform*的输出，它估计偏差校正效应大小为*d* = 0.0126。这个效应大小与0没有显著差异，*p* = 0.3857，因此，这个偏差检测技术正确地表明，即使所有效应都是显著的，这些研究的集合也没有提供拒绝0的元分析效应量估计的充分理由。


```{r}
puniform::puniform(m1i = metadata$m1, m2i = metadata$m2, n1i = metadata$n1, 
  n2i = metadata$n2, sd1i = metadata$sd1, sd2i = metadata$sd2, side = "right")
```

An alternative technique that also meta-analyzes the *p*-values from individual studies is a *z*-curve analysis, which is a meta-analysis of observed power ([@bartos_z-curve20_2020; @brunner_estimating_2020]; for an example, see [@sotola_garbage_2022]). Like a traditional meta-analysis, *z*-curve analysis transforms observed test results (*p*-values) into *z*-scores. In an unbiased literature where the null hypothesis is true, we should observe approximately $\alpha$% significant results. If the null is true, the distribution of *z*-scores is centered on 0. *Z*-curve analysis computes absolute *z*-values, and therefore $\alpha$% of *z*-scores should be larger than the critical value (1.96 for a 5% alpha level). In Figure \@ref(fig:zcurveunbiasednull) *z*-scores for 1000 studies are plotted, with a true effect size of 0, where exactly 5% of the observed results are statistically significant. 
一个也对个体的研究中的*p*值进行元分析的替代技术是*z*值曲线分析，它是观察功效的元分析 [@bartos_z-curve20_2020; @brunner_estimating_2020]（一个例子请参见[@sotola_garbage_2022]）。与传统的元分析类似，*z*值曲线分析将观察到的检验结果（*p*值）转化为*z*分数。在一个无偏的文献中，如果零假设成立，我们应该观察到大约$\alpha$%的显著结果。如果零假设成立，则*z*分数的分布以0为中心。*Z*值曲线分析计算绝对值的*z*值，因此应有$\alpha$%的*z*分数大于临界值（5%的显著性水平下为1.96）。在图\@ref(fig:zcurveunbiasednull)中，绘制了1000个研究的*z*分数，真实效应大小为0，观察到的结果中恰好有5%是统计显著的。

(ref:zcurveunbiasednull)*Z*曲线对1000项研究进行分析，真实效应大小为0，不存在发表偏倚。

```{r zcurveunbiasednull, echo = FALSE, cache = TRUE, fig.cap="(ref:zcurveunbiasednull)"}
set.seed(1)

nsims <- 1000 # number of simulated experiments
pub.bias <- 0.05 # set percentage of significant results in the literature

m1 <- 0 # too large effects will make non-significant results extremely rare
sd1 <- 1
m2 <- 0
sd2 <- 1
metadata.sig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                           n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)
metadata.nonsig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                              n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)

# simulate significant effects in the expected direction
if(pub.bias > 0){
  for (i in 1:nsims*pub.bias) { # for each simulated experiment
    p <- 1 # reset p to 1
    n <- 100 # n based on truncated normal
    while (p > 0.025) { # continue simulating as along as p is not significant
      x <- rnorm(n = n, mean = m1, sd = sd1) 
      y <- rnorm(n = n, mean = m2, sd = sd2) 
      p <- t.test(x, y, alternative = "greater", var.equal = TRUE)$p.value
    }
    metadata.sig[i, 1] <- mean(x)
    metadata.sig[i, 2] <- mean(y)
    metadata.sig[i, 3] <- sd(x)
    metadata.sig[i, 4] <- sd(y)
    metadata.sig[i, 5] <- n
    metadata.sig[i, 6] <- n
    out <- t.test(x, y, var.equal = TRUE)
    metadata.sig[i, 7] <- out$p.value
    metadata.sig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
  }}

# simulate non-significant effects (two-sided)
if(pub.bias < 1){
  for (i in 1:nsims*(1-pub.bias)) { # for each simulated experiment
    p <- 0 # reset p to 1
    n <- 100
    while (p < 0.05) { # continue simulating as along as p is significant
      x <- rnorm(n = n, mean = m1, sd = sd1) # produce  simulated participants
      y <- rnorm(n = n, mean = m2, sd = sd2) # produce  simulated participants
      p <- t.test(x, y, var.equal = TRUE)$p.value
    }
    metadata.nonsig[i, 1] <- mean(x)
    metadata.nonsig[i, 2] <- mean(y)
    metadata.nonsig[i, 3] <- sd(x)
    metadata.nonsig[i, 4] <- sd(y)
    metadata.nonsig[i, 5] <- n
    metadata.nonsig[i, 6] <- n
    out <- t.test(x, y, var.equal = TRUE)
    metadata.nonsig[i, 7] <- out$p.value
    metadata.nonsig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
  }}

# Combine significant and non-significant effects
metadata <- rbind(metadata.nonsig, metadata.sig)

# Use escalc to compute effect sizes
metadata <- escalc(n1i = n1, n2i = n2, m1i = m1, m2i = m2, sd1i = sd1, 
                   sd2i = sd2, measure = "SMD", data = metadata[complete.cases(metadata),])

# Perform the z-curve analysis using the z-curve package
z_res <- zcurve::zcurve(p = metadata$pvalues, method = "EM", bootstrap = 1000)
par(bg = backgroundcolor)
plot(z_res, annotation = TRUE, CI = TRUE)

```

If there is a true effect, the distribution of *z*-scores shifts away from 0, as a function of the statistical power of the test. The higher the power, the further to the right the distribution of *z*-scores will be located. For example, when examining an effect with 66% power, an unbiased distribution of *z*-scores, computed from observed *p*-values, looks like the distribution in Figure \@ref(fig:zcurveunbiasedalternative). 
如果存在真实效应，*z*值的分布会随着统计功效的变化而发生变化，*z*值的分布会偏离0。统计功效越高，*z*值的分布就越偏向右边。例如，当检查具有66%统计功效的效应时，从观察到的*p*值计算得出的不偏*z*值分布如图\@ref(fig:zcurveunbiasedalternative)所示。

(ref:zcurveunbiasedalternative) *Z*曲线分析1000项研究，在没有发表偏差的独立*t*检验中，每个条件的真实效应大小为*d* = 0.37和*n* = 100。

```{r zcurveunbiasedalternative, echo = FALSE, cache = TRUE, fig.cap="(ref:zcurveunbiasedalternative)"}
set.seed(5)

nsims <- 1000 # number of simulated experiments
pub.bias <- 0.66 # set percentage of significant results in the literature

m1 <- 0.37 # too large effects will make non-significant results extremely rare
sd1 <- 1
m2 <- 0
sd2 <- 1
metadata.sig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                           n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)
metadata.nonsig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                              n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)

# simulate significant effects in the expected direction
if(pub.bias > 0){
  for (i in 1:nsims*pub.bias) { # for each simulated experiment
    p <- 1 # reset p to 1
    n <- 100 # n based on truncated normal
    while (p > 0.025) { # continue simulating as along as p is not significant
      x <- rnorm(n = n, mean = m1, sd = sd1) 
      y <- rnorm(n = n, mean = m2, sd = sd2) 
      p <- t.test(x, y, alternative = "greater", var.equal = TRUE)$p.value
    }
    metadata.sig[i, 1] <- mean(x)
    metadata.sig[i, 2] <- mean(y)
    metadata.sig[i, 3] <- sd(x)
    metadata.sig[i, 4] <- sd(y)
    metadata.sig[i, 5] <- n
    metadata.sig[i, 6] <- n
    out <- t.test(x, y, var.equal = TRUE)
    metadata.sig[i, 7] <- out$p.value
    metadata.sig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
  }}

# simulate non-significant effects (two-sided)
if(pub.bias < 1){
  for (i in 1:nsims*(1-pub.bias)) { # for each simulated experiment
    p <- 0 # reset p to 1
    n <- 100
    while (p < 0.05) { # continue simulating as along as p is significant
      x <- rnorm(n = n, mean = m1, sd = sd1) # produce  simulated participants
      y <- rnorm(n = n, mean = m2, sd = sd2) # produce  simulated participants
      p <- t.test(x, y, var.equal = TRUE)$p.value
    }
    metadata.nonsig[i, 1] <- mean(x)
    metadata.nonsig[i, 2] <- mean(y)
    metadata.nonsig[i, 3] <- sd(x)
    metadata.nonsig[i, 4] <- sd(y)
    metadata.nonsig[i, 5] <- n
    metadata.nonsig[i, 6] <- n
    out <- t.test(x, y, var.equal = TRUE)
    metadata.nonsig[i, 7] <- out$p.value
    metadata.nonsig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
  }}

# Combine significant and non-significant effects
metadata <- rbind(metadata.nonsig, metadata.sig)

# Use escalc to compute effect sizes
metadata <- escalc(n1i = n1, n2i = n2, m1i = m1, m2i = m2, sd1i = sd1, 
                   sd2i = sd2, measure = "SMD", data = metadata[complete.cases(metadata),])

# Perform the z-curve analysis using the z-curve package
z_res <- zcurve::zcurve(p = metadata$pvalues, method = "EM", bootstrap = 1000)
par(bg = backgroundcolor)
plot(z_res, annotation = TRUE, CI = TRUE)

```

In any meta-analysis the studies that are included will differ in their statistical power, and their true effect size (due to heterogeneity). *Z*-curve analysis uses mixtures of normal distributions centered at means 0 to 6 to fit a model of the underlying effect sizes that best represents the observed results in the included studies (for the technical details, see @bartos_z-curve20_2020. The *z*-curve then aims to estimate the average power of the set of studies, and then calculates the *observed discovery rate* (ODR: the percentage of significant results, or the observed power), the *expected discovery rate* (EDR: the proportion of the area under the curve on the right side of the significance criterion) and the expected replication rate (ERR: the expected proportion of successfully replicated significant studies from all significant studies). The *z*-curve is able to correct for selection bias for positive results (under specific assumptions), and can estimate the EDR and ERR using only the significant *p*-values.
在任何元分析中，包括的研究将在其统计功效和真实效应大小方面存在差异（由于异质性）。*Z*值曲线分析使用以0至6为中心的正态分布混合来拟合最能代表所包括研究的观察结果的潜在效应大小的模型（有关技术细节，请参见@bartos_z-curve20_2020）。然后，*z*值曲线旨在估计研究集的平均功效，然后计算*观察发现率*（ODR：显著结果的百分比或观察到的功效）、*预期发现率*（EDR：显著性水平右侧曲线下面积的比例）和预期复制率（ERR：所有显著研究中成功复制显著研究的预期比例）。*Z*值曲线能够在特定假设下纠正对正面结果的选择偏倚，并可以仅使用显著的*p*值来估计EDR和ERR。

To examine the presence of bias, it is preferable to submit non-significant and significant *p*-values to a *z*-curve analysis, even if only the significant *p*-values are used to produce estimates. Publication bias can then be examined by comparing the ODR to the EDR. If the percentage of significant results in the set of studies (ODR) is much higher than the expected discovery rate (EDR), this is a sign of bias. If we analyze the same set of biased studies as we used to illustrate the bias detection techniques discussed above, *z*-curve analysis should be able to indicate the presence of bias. We can perform the *z*-curve with the following code: 
为了检查是否存在偏差，最好将非显著性和显著性*p*值一起提交到*z*值曲线分析中，即使仅使用显著性*p*值进行估计。然后可以通过比较ODR和EDR来检查发表偏倚。如果研究中显著结果的百分比（ODR）远高于预期发现率（EDR），则存在偏差的迹象。如果我们使用上面讨论偏差检测技术的偏倚样本集进行分析，*z*值曲线分析应该能够指示是否存在偏倚。我们可以使用以下代码执行*z*值曲线分析：


```{r, eval = FALSE}
z_res <- zcurve::zcurve(p = metadata$pvalues, method = "EM", bootstrap = 1000)
summary(z_res, all = TRUE)
plot(z_res, annotation = TRUE, CI = TRUE)

```


```{r, cache = TRUE, echo = FALSE}
# Perform the z-curve analysis using the z-curve package

set.seed(5)
nsims <- 100 # number of simulated experiments
pub.bias <- 0.96 # set percentage of significant results in the literature

m1 <- 0 # too large effects will make non-significant results extremely rare
sd1 <- 1
m2 <- 0
sd2 <- 1
metadata.sig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                           n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)
metadata.nonsig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                              n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)

# simulate significant effects in the expected direction
if(pub.bias > 0){
for (i in 1:nsims*pub.bias) { # for each simulated experiment
  p <- 1 # reset p to 1
  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100)) # n based on truncated normal
  while (p > 0.025) { # continue simulating as along as p is not significant
    x <- rnorm(n = n, mean = m1, sd = sd1) 
    y <- rnorm(n = n, mean = m2, sd = sd2) 
    p <- t.test(x, y, alternative = "greater", var.equal = TRUE)$p.value
  }
  metadata.sig[i, 1] <- mean(x)
  metadata.sig[i, 2] <- mean(y)
  metadata.sig[i, 3] <- sd(x)
  metadata.sig[i, 4] <- sd(y)
  metadata.sig[i, 5] <- n
  metadata.sig[i, 6] <- n
  out <- t.test(x, y, var.equal = TRUE)
  metadata.sig[i, 7] <- out$p.value
  metadata.sig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
}}

# simulate non-significant effects (two-sided)
if(pub.bias < 1){
for (i in 1:nsims*(1-pub.bias)) { # for each simulated experiment
  p <- 0 # reset p to 1
  n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100))
  while (p < 0.05) { # continue simulating as along as p is significant
    x <- rnorm(n = n, mean = m1, sd = sd1) # produce  simulated participants
    y <- rnorm(n = n, mean = m2, sd = sd2) # produce  simulated participants
    p <- t.test(x, y, var.equal = TRUE)$p.value
  }
  metadata.nonsig[i, 1] <- mean(x)
  metadata.nonsig[i, 2] <- mean(y)
  metadata.nonsig[i, 3] <- sd(x)
  metadata.nonsig[i, 4] <- sd(y)
  metadata.nonsig[i, 5] <- n
  metadata.nonsig[i, 6] <- n
  out <- t.test(x, y, var.equal = TRUE)
  metadata.nonsig[i, 7] <- out$p.value
  metadata.nonsig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
}}

# Combine significant and non-significant effects
metadata <- rbind(metadata.nonsig[complete.cases(metadata.nonsig),], metadata.sig[complete.cases(metadata.sig),])

z_res <- zcurve::zcurve(p = metadata$pvalues, method = "EM", bootstrap = 1000)
summary(z_res, all = TRUE)
par(bg = backgroundcolor)
plot(z_res, annotation = TRUE, CI = TRUE)
```

We see that the distribution of *z*-scores looks peculiar. Most expected *z*-scores between 0 and 1.96 are missing. 96 out of 100 studies were significant, which makes the observed discovery rate (ODR), or observed power (across all these studies with different sample sizes) 0.96, 95% CI[0.89; 0.99]. The expected discovery rate (EDR) is only `r round(z_res$coefficients[2], 3)`, which differs statistically from the observed discovery rate, as indicated by the fact that the confidence interval of the EDR does not overlap with the ODR of 0.96. This means there is clear indication of selection bias based on the *z*-curve analysis. The expected replicability rate for these studies is only `r round(z_res$coefficients[1], 3)`, which is in line with the expectation that we will only observe 5% Type 1 errors, as there was no true effect in this simulation. Thus, even though we only entered significant *p*-values, *z*-curve analysis correctly suggests that we should not expect these results to replicate at a higher frequency than the Type 1 error rate.
我们可以看到，*z*值得分的分布看起来很奇怪。大部分期望的*z*得分在0和1.96之间的值都不见了。96个研究中有95个研究是显著的，这使得观察发现率（ODR）或（在这些具有不同样本量的研究中）观察到的功效为0.96，95% CI[0.89; 0.99]。期望的发现率（EDR）只有`r round(z_res$coefficients[2], 3)`，这与观察到的发现率存在统计学差异，这表明EDR的置信区间与0.96的ODR不重叠。这意味着基于*z*值曲线分析存在明显的选择偏倚。这些研究的预期可复制性率只`r round(z_res$coefficients[1], 3)`，这与我们只会观察到5%的Type 1错误的期望相一致，因为在这个模拟中没有真实的效应。因此，即使我们只输入了显著的*p*值，*z*值曲线分析正确地表明我们不应该期望这些结果的复制率高于Type 1错误率。

## Conclusion
## 结论
发表偏倚是科学界的一个大问题。几乎所有对科学文章中主要假设检验进行的元分析都存在此问题，因为主要假设检验具有统计显著性的文章更可能成功发表。未经偏倚校正的元分析效应量估计会高估真实的效应量，而偏倚校正过的效应量估计可能仍然不够准确。鉴于科学文献已经受到发表偏倚的影响，我们无法确知我们对文献中元分析效应量估计的计算是否准确。发表偏倚会放大效应量估计，而其影响程度是未知的，已经证实了有几个案例的真实效应量为零。虽然本章提到的发表偏倚检验可能无法提供无偏效应量的确定性，但它们可以作为标志来检测偏倚是否存在，并提供校正后的效应量估计。如果发表偏倚的基本模型是正确的，那么校正后的估计值会更接近真实值。
Publication bias is a big problem in science. It is present in almost
all meta-analyses performed on the primary hypothesis test in
**scientific articles**, because these articles are much more likely to
be submitted and accepted for publication if the primary hypothesis test
is statistically significant. **Meta-analytic effect size estimates that
are not adjusted for bias** will almost always overestimate the true
effect size, **and bias-adjusted effect sizes might still be
misleading**. Having messed up the scientific literature through
publication bias, there is no way for us to know whether we are
computing accurate meta-analytic effect sizes estimates from the
literature. Publication bias inflates the effect size estimate to an
unknown extent, and there have already have been several cases where the
true effect size turned out to be zero. The publication bias tests in
this chapter might not provide certainty about the unbiased effect size,
but they can function as a red flag to indicate when bias is present,
and provide adjusted estimates that, if the underlying model of
publication bias is correct, might well be closer to the truth.

目前学术界对于出版偏倚检验的研究非常活跃。存在许多不同的检验方法，使用之前你需要仔细检查每种检验的假设条件。当存在较大的异质性时，大多检验都不太可靠，而异质性的出现是非常有可能的。元分析应始终检查是否存在发表偏倚，最好使用多种发表偏倚检验方法。因此不仅要编码效应量，还应编码检验的统计量或p值，这对研究非常有用。本章讨论的偏倚检验技术没有一个是完美的，但相比于单纯地解释元分析中未校正的效应量估计，它们更为可靠。
There is a lot of activity in the literature on tests for publication
bias. There are many different tests, and you need to carefully check
the assumptions of each test before applying it. Most tests don't work
well when there is large heterogeneity, and heterogeneity is quite
likely. A meta-analysis should always examine whether there is
publication bias, preferably using multiple publication bias tests, and
therefore it is useful to not just code effect sizes, but also code test
statistics or *p*-values. None of the bias detection techniques
discussed in this chapter will be a silver bullet, but they will be
better than naively interpreting the uncorrected effect size estimate
from the meta-analysis.

关于发表偏倚检验的另一个开放学习资源，请参阅[Doing Meta-Analysis in
R](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pub-bias.html).

For another open educational resource on tests for publication bias,
see[Doing Meta-Analysis in
R](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pub-bias.html).

## Test Yourself
## 自我测验

**Q1**: What happens when there is publication bias because researchers
only publish statistically significant results (*p* \< $\alpha$), and
you calculate the effect size in a meta-analysis?
**Q1**:当出现由于研究人员只发表具有统计显著性(*p*&nbsp\<&nbsp$\alpha$)的结果而导致的发表偏倚时，计算元分析中的效应量会出现什么情况？

A)  The meta-analytic effect size estimate is **identical** whether
    there is publication bias (where researchers publish only effects
    with *p* \< $\alpha$) or no publication bias.
    无论是否存在发表偏倚（即研究人员只发表*p*&nbsp\<&nbsp$alpha$的显著结果），元分析的效应量估计值都是**相同的**。
B)  The meta-analytic effect size estimate is **closer to the true
    effect size** when there is publication bias (where researchers
    publish only effects with *p* \< $\alpha$) compared to when there is
    no publication bias.
    当存在发表偏倚（研究人员只发表*p*&nbsp\<&nbsp$alpha$的显著结果）时，元分析效应量估计值会比不存在发表偏倚时**更接近真实效应量**。
C)  The meta-analytic effect size estimate is **inflated** when there is
    publication bias (where researchers publish only effects with *p*
    \<$\alpha$) compared to when there is no publication bias.
    当存在发表偏倚（研究人员只发表*p*&nbsp\<&nbsp$alpha$的显著结果）时，元分析的效应量估计值会比不存在发表偏差时**更高**。
D)  The meta-analytic effect size estimate is **lower** when there is
    publication bias (where researchers publish only effects with *p*
    \<$\alpha$) compared to when there is no publication bias.
    当存在发表偏倚（研究人员只发表*p*&nbsp\<&nbsp$alpha$的显著结果）时，元分析效应量估计会比没有发表偏倚时**更低**。

**Q2**: The forest plot in the figure below looks quite peculiar. What
do you notice? **Q2**: 下图中的森林图看起来相当奇怪，你注意到了什么？

```{r metasimq2, echo = FALSE}
set.seed(27988)
nsims <- 10 # number of simulated experiments
pub.bias <- 1 # set percentage of significant results in the literature

m1 <- 0 # too large effects will make non-significant results extremely rare
sd1 <- 1
m2 <- 0
sd2 <- 1
metadata.sig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                           n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)
metadata.nonsig <- data.frame(m1 = NA, m2 = NA, sd1 = NA, sd2 = NA, 
                              n1 = NA, n2 = NA, pvalues = NA, pcurve = NA)

# simulate significant effects in the expected direction
if(pub.bias > 0){
  for (i in 1:nsims*pub.bias) { # for each simulated experiment
    p <- 1 # reset p to 1
    n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100)) # n based on truncated normal
    while (p > 0.025) { # continue simulating as along as p is not significant
      x <- rnorm(n = n, mean = m1, sd = sd1) 
      y <- rnorm(n = n, mean = m2, sd = sd2) 
      p <- t.test(x, y, alternative = "greater", var.equal = TRUE)$p.value
    }
    metadata.sig[i, 1] <- mean(x)
    metadata.sig[i, 2] <- mean(y)
    metadata.sig[i, 3] <- sd(x)
    metadata.sig[i, 4] <- sd(y)
    metadata.sig[i, 5] <- n
    metadata.sig[i, 6] <- n
    out <- t.test(x, y, var.equal = TRUE)
    metadata.sig[i, 7] <- out$p.value
    metadata.sig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
  }}

# simulate non-significant effects (two-sided)
if(pub.bias < 1){
  for (i in 1:nsims*(1-pub.bias)) { # for each simulated experiment
    p <- 0 # reset p to 1
    n <- round(truncnorm::rtruncnorm(1, 20, 1000, 100, 100))
    while (p < 0.05) { # continue simulating as along as p is significant
      x <- rnorm(n = n, mean = m1, sd = sd1) # produce  simulated participants
      y <- rnorm(n = n, mean = m2, sd = sd2) # produce  simulated participants
      p <- t.test(x, y, var.equal = TRUE)$p.value
    }
    metadata.nonsig[i, 1] <- mean(x)
    metadata.nonsig[i, 2] <- mean(y)
    metadata.nonsig[i, 3] <- sd(x)
    metadata.nonsig[i, 4] <- sd(y)
    metadata.nonsig[i, 5] <- n
    metadata.nonsig[i, 6] <- n
    out <- t.test(x, y, var.equal = TRUE)
    metadata.nonsig[i, 7] <- out$p.value
    metadata.nonsig[i, 8] <- paste0("t(", out$parameter, ")=", out$statistic)
  }}

# Combine significant and non-significant effects
metadata <- rbind(metadata.nonsig[complete.cases(metadata.nonsig),],
                  metadata.sig[complete.cases(metadata.sig),])

# Use escalc to compute effect sizes
metadata <- metafor::escalc(n1i = n1, n2i = n2, m1i = m1, m2i = m2, sd1i = sd1, 
                            sd2i = sd2, measure = "SMD", data = metadata)
# add se for PET-PEESE analysis
metadata$sei <- sqrt(metadata$vi)

#Perform meta-analysis
result.biased <- metafor::rma(yi, vi, data = metadata)
par(bg = backgroundcolor)
metafor::forest(result.biased)
```

A)  All effect sizes are quite similar, suggesting large sample sizes
    and highly accurate effect size measures.
    所有效应量都非常相似，表明样本量很大且对效应量的测量非常准确。

B)  The studies look as if they were designed based on perfect a-priori
    power analyses, all yielding just significant results.
    这些研究看起来好像是基于完美的*先验功效分析/先验样本量估算*设计的，所有结果都显著。

C)  The studies have confidence intervals that only just fail to include
    0, suggesting most studies are only just statistically significant.
    This suggests publication bias.
    这些研究的置信区间仅仅没有包含0，这意味着大多数研究仅仅具有较小的统计显著性，即存在发表偏倚。

D)  All effects are in the same direction, which suggests that one-sided
    tests have been performed, even though these might not have been
    preregistered.
    所有效应方向相同，这表明可能进行了单侧检验，尽管这些检验可能没有进行预注册。

**Q3**: Which statement is true? **Q3**:以下哪个表述是正确的？

A)  With extreme publication bias, all individual studies in a
    literature can be significant, but the standard errors are so large
    that the meta-analytic effect size estimate is not significantly
    different from 0.
    在极端的发表偏倚下，文献中所有单独的研究都显著，但标准误差非常大，导致元分析的效应量估计值与0之间的差异不显著。
B)  With extreme publication bias, all individual studies in a
    literature can be significant, but the meta-analytic effect size
    estimate will be severely inflated, giving the impression there is
    overwhelming support for $H_1$ when actually the true effect size is
    either small, or even 0.
    在极端的发表偏倚下，文献中所有单独的研究都显著，但元分析效应量估计值被严重夸大，给人留下支持$H_1$的压倒性印象，而实际上真实效应可能很小，甚至可能为0。
C)  With extreme publication bias, all individual studies are
    significant, but meta-analytic effect size estimates are
    automatically corrected for publication bias in most statistical
    packages, and the meta-analytic effect size estimate is therefore
    quite reliable.
    在极端的发表偏倚下，所有单独的研究都显著，但大多数统计软件包进行元分析效应量估计时会自动校正发表偏倚，因此元分析效应量估计值是可靠的。
D)  Regardless of whether there is publication bias, the meta-analytic
    effect size estimate is severely biased, and it should never be
    considered a reliable estimate of the population.
    无论是否存在发表偏倚，元分析的效应量估计都存在严重偏差，因此不能将其是为总体的可靠估计。

**Q4**: Which statement is true based on the plot below, visualizing a
PET-PEESE meta-regression? 根据下面基于 PET-PEESE
的元回归分析图表，以下哪个陈述是正确的？

(ref:petpeeseq4lab) Funnel plot with PETPEESE regression lines for the
same studies as in Q2. 对Q2中的研究，使用PET-PEESE回归线绘制漏斗图。

```{r petpeeseq4, echo = FALSE, fig.cap="(ref:petpeeseq4lab)"}

# PET
PET <- metafor::rma(yi = yi, sei = sei, mods = ~sei, data = metadata, method = "FE")

# PEESE
PEESE <- metafor::rma(yi = yi, sei = sei, mods = ~I(sei^2), data = metadata, method = "FE")

# Funnel Plot 
par(bg = backgroundcolor)
metafor::funnel(result.biased, level = 0.95, refline = 0, main = paste("FE d =", round(result.biased$b[1],2),"PET d =", round(PET$b[1],2),"PEESE d =", round(PEESE$b[1],2)))
abline(v = result.biased$b[1], lty = "dashed") #draw vertical line at meta-analytic effect size estimate
points(x = result.biased$b[1], y = 0, cex = 1.5, pch = 17) #draw point at meta-analytic effect size estimate
# PET PEESE code below is adapted from Joe Hilgard: https://github.com/Joe-Hilgard/PETPEESE/blob/master/PETPEESE_functions.R
# PEESE line and point
sei <- (seq(0, max(sqrt(result.biased$vi)), .001))
vi <- sei^2
yi <- PEESE$b[1] + PEESE$b[2]*vi
grid <- data.frame(yi, vi, sei)
lines(x = grid$yi, y = grid$sei, typ = 'l') # add line for PEESE
points(x = (PEESE$b[1]), y = 0, cex = 1.5, pch = 5) # add point estimate for PEESE
# PET line and point
abline(a = -PET$b[1]/PET$b[2], b = 1/PET$b[2]) # add line for PET
points(x = PET$b[1], y = 0, cex = 1.5) # add point estimate for PET
segments(x0 = PET$ci.lb[1], y0 = 0, x1 = PET$ci.ub[1], y1 = 0, lty = "dashed") #Add 95% CI around PET

```

A)  Using PET-PEESE meta-regression we can show that the true effect
    size is d = 0 (based on the PET estimate).
    通过PET-PEESE元回归，我们可以得知真实效应大小为d =
    0（基于PET估计）。
B)  Using PET-PEESE meta-regression we can show that the true effect
    size is d = `r round(PEESE$b[1],2)` (based on the PEESE estimate).
    通过PET-PEESE元回归，我们可以得知真实效应大小为 d =
    `r round(PEESE$b[1],2)` （基于PEESE估计）。
C)  Using PET-PEESE meta-regression we can show that the true effect
    size is d = `r round(result.biased$b,2)` (based on the normal
    meta-analytic effect size estimate).
    通过PET-PEESE元回归，我们可以得知真实效应大小为d=`rround(result.biased$b,2)`（基于一般的元分析效应量估计）。
D)  The small sample size (10 studies) means PET has very low power to
    reject the null, and therefore it is not a reliable indicator of
    bias - but there might be reason to worry.
    由于样本量很小（10个研究），PET的统计检验力很低，无法可靠地拒绝零假设，因此它不是偏倚的可靠指标，但仍有偏倚存在的风险。

**Q5**: Take a look at the figure and output table of the *p*-curve app
below, which gives the results for the studies in Q2. Which
interpretation of the output is correct?
如下是p-curve应用程序输出的图表，其中给出了Q2中的研究结果。以下哪种解释是正确的？

```{r, echo = FALSE, eval = FALSE}
cat(metadata$pcurve,sep = "\n")
```

(ref:pcurveresultq5lab) Result of the p-curve analysis of the biased
studies in Q2. Q2中偏倚研究的p-curve分析结果。

```{r pcurveresultq5, fig.margin = FALSE, echo = FALSE, fig.cap="(ref:pcurveresultq5lab)"}
knitr::include_graphics("images/pcurveresultq5.png")
```

A)  Based on the continuous Stouffer's test for the full *p*-curve, we
    cannot reject a *p*-value distribution expected under $H_0$, and we
    can reject a *p*-value distribution as expected if $H_1$ is true and
    studies had 33% power.
    基于对p-curve进行的连续Stouffer's检验，我们无法拒绝在H0假设下预期的p值分布，但在33%的统计检验力下，可以拒绝在
    H1假设为真时预期的 p值分布。

B)  Based on the continuous Stouffer's test for the full *p*-curve, we
    can conclude the observed *p*-value distribution is not skewed
    enough to be interpreted as the presence of a true effect size,
    therefore the theory used to deduce these studies is incorrect.
    基于对p-curve进行的连续Stouffer's检验，我们可以得出结论：观察到的p值分布的倾斜程度不足以解释真实效应量的大小，因此用于推断这些研究的理论是错误的。

C)  Based on the continuous Stouffer's test for the full *p*-curve, we
    can conclude the observed *p*-value distribution is skewed enough to
    be interpreted in line with a *p*-value distribution as expected if
    $H_1$ is true and studies had 33% power.
    基于对p-curve进行的连续Stouffer's检验，我们可以得出结论：观察到的p值分布倾斜程度足以解释为H1为真且研究统计检验力为33%时预期的p值分布。

D)  Based on the continuous Stouffer's test for the full *p*-curve, we
    can conclude the observed *p*-value distribution is flatter than we
    would expect if the studies had 33% power, and therefore, we can
    conclude these studies are based on fabricated data.
    基于对p-curve进行的连续Stouffer's检验，我们可以得出结论：观察到的p值分布比研究统计检验力为33%时预期的p值分布更平坦，因此可以推测这些研究伪造了数据。

**Q6**: The true effect size in the studies simulated in Q2 is 0 - there
is no true effect. Which statement about the *z*-curve analysis below is
true? 在 Q2
模拟的研究中，真实效应量为0，即不存在真实效应。下面关于z-curve的分析，哪个陈述是正确的？

```{r zcurveq6, cache = TRUE, echo = FALSE}
# Perform the z-curve analysis using the z-curve package
z_res <- zcurve::zcurve(p = metadata$pvalues, method = "EM", bootstrap = 1000)
par(bg = backgroundcolor)
plot(z_res, annotation = TRUE, CI = TRUE)
```

A)  The expected discovery rate and the expected replicability rate are
    both statistically significant, and therefore we can expect the
    observed effects to successfully replicate in future studies.
    预期的发现率和可重复率都具有统计显著性，因此可以预计这些观测到的效应能够在未来的研究中成功复制。

B)  Despite the fact that the average observed power (the observed
    discovery rate) is 100%, *z*-curve correctly predicts the expected
    replicability rate (which is 5%, as only Type 1 errors will be
    statistically significant).
    尽管观测到的平均功效（观测到的发现率）为100％，但z-curve正确预测了预期的可重复率（仅为5%，只有当研究发生第一类错误时，结果才会在统计学上显著）。

C)  *Z*-curve is not able to find an indication of bias, as the expected
    discovery rate and the expected replicability rate do not differ
    from each other statistically.
    由于预期的发现率和可重复率在统计上没有显著差异，因此Z-curve无法识别偏差。

D)  Although the observed discovery rate is 1 (indicating an observed
    power of 100%) the confidence interval ranges from 0.66 to 1, which
    indicates that the studies could have a lower but more realistic
    power, and the fact that 100% of the results were significant could
    have happened by chance.
    虽然观测的发现率为1（表示观测到的功效为100%），但置信区间范围为0.66-1，这表明这些研究实际上的功效更低，100%的结果显著可能只是偶然。

**Q7**: We did not yet perform a trim and fill analysis, and given the
analyses above (e.g., the *z*-curve analysis), which statement is true?
我们尚未进行修剪和填补分析，鉴于上述分析（例如z-curve分析），以下哪个陈述是正确的？

A)  The trim-and-fill method would most likely not indicate any missing
    studies to 'fill'.
    修剪和填补方法可能不会指出有需要"填补"的缺失研究。

B)  The trim-and-fill method has known low power to detect bias, and
    would contradict the *z*-curve or *p*-curve analysis reported above.
    修剪和填补方法检测偏倚的功效较弱，而且可能与上面提到的z-curve或p-curve分析相矛盾。

C)  The trim-and-fill analysis would indicate bias, but so did the
    *p*-curve and *z*-curve analysis, and the adjusted effect size
    estimate by trim-and-fill does not adequately correct for bias, so
    the analysis would not add anything.
    修剪和填补分析可以检测偏倚，正如p-curve和z-curve分析，而且修剪和填补分析校正后的效应量估计并不能充分纠正偏倚，因此这个分析没有任何作用。

D)  The trim-and-fill method provides a reliable estimate of the true
    effect size, which is not provided by any of the other methods
    discussed so far, and therefore it should be reported alongside
    other bias detection tests.
    修剪和填补方法可以对真实效应量进行可靠估计，而上述的其他方法都做不到，因此应该将其与其他的偏倚检验一起报告。

**Q8**: Publication bias is defined as the practice of selectively
submitting and publishing scientific research. Throughout this chapter,
we have focussed on selectively submitting *significant* results. Can
you think of a research line or a research question where researchers
might prefer to selectively publish *non-significant* results?
发表偏倚被定义为选择性提交和发表科学研究。在本章中，我们关注的是选择性提交显著结果。你能想到某个研究领域或研究问题，其研究人员更喜欢选择性地发表不显著的结果吗？

### Open Questions
### 开放性问题
1.  What is the idea behind the GRIM test? GRIM检验背后的思想是什么？
2.  What is the definition of 'publication bias'? 发表偏倚的定义是什么？
3.  What is the file-drawer problem? 什么是"文件抽屉问题"？
4.  In a funnel plot, what is true for studies that fall inside the
    funnel (when it is centered on 0)?
    在漏斗图中，对于落在漏斗中心（居中处于0）的研究，代表了什么？
5.  What is true for the trim-and-fill approach with respect to its
    ability to detect and correct effect size estimates?
    修剪和填补方法在检测和纠正效应量估计方面有何特点？
6.  When using the PET-PEESE approach, what is important to consider
    when the meta-analysis has a small number of studies?
    当使用PET-PEESE方法时，如果元分析中的研究数量很少，什么是需要重点考虑的？
7.  What conclusions can we draw from the 2 tests that are reported in a
    p-curve analysis?
    从p-curve分析报告的两个检验中，我们可以得出哪些结论？
